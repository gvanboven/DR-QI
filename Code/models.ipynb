{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In this file we present the model code for our ACL 2022 submission.\n",
    "\n",
    "In this file we load preprocessed data, and train 3 models: \n",
    "* a baseline model, \n",
    "* the demographic model and \n",
    "* the encoded demographic model. \n",
    "\n",
    "For this latter model we use an Autoencoder to encode demograhic information, which is also presented in this notebook.\n",
    "\n",
    "Please refer to section 4 of our paper for a more elaborate description of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "gather": {
     "logged": 1636977932222
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, activations, optimizers, losses\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927430866
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "# MODEL HYPERPARAMETERS\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 5e-5\n",
    "LAYER_DROPOUT = 0.2\n",
    "VECTOR_LENGTH = 49\n",
    "MAX_LENGTH=54\n",
    "BATCH_SIZE=64\n",
    "\n",
    "#AUTOENCODER HYPERPARAMETERS\n",
    "ENCODING_DIM = 32\n",
    "LEARNING_RATE_ENCODER = 5e-3\n",
    "\n",
    "#UPV LABEL SETTINGS\n",
    "MIN_T3_OCCURENCECS = 10\n",
    "EXCLUDED_LABELS = ['functionality', 'performance (personal)']\n",
    "\n",
    "#DATA PATHS\n",
    "DATA_PATH=''\n",
    "MODELS_PATH = 'Models'\n",
    "INPUT_DATA_PATH = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927437423
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#open preprocessed and split input data\n",
    "with open(INPUT_DATA_PATH) as json_file:\n",
    "        inputs = json.load(json_file)\n",
    "\n",
    "#open training data\n",
    "train_text                  = inputs['train_text']\n",
    "train_gold_labels           = inputs['train_gold_labels']\n",
    "train_demographic_vectors   = inputs['train_demographic_vectors']\n",
    "\n",
    "#open validation data\n",
    "val_text                    = inputs['val_text']\n",
    "val_gold_labels             = inputs['val_gold_labels']\n",
    "val_demographic_vectors     = inputs['val_demographic_vectors']\n",
    "\n",
    "#open test data\n",
    "test_text                   = inputs['test_text']\n",
    "test_gold_labels            = inputs['test_gold_labels']\n",
    "test_demographic_vectors    = inputs['test_demographic_vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927438488
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#load DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927439072
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def batch_encode(tokenizer, texts, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    A function that encodes a batch of texts and returns the texts'\n",
    "    corresponding encodings and attention masks that are ready to be fed \n",
    "    into a pre-trained transformer model.\n",
    "    \n",
    "    Input:\n",
    "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
    "        - texts:       List of tuples where each tuple represents a label and an interview extract\n",
    "        - batch_size:  Integer controlling number of texts in a batch\n",
    "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
    "    Output:\n",
    "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
    "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, truncation=True, padding='max_length', max_length=max_length,\\\n",
    "                            return_attention_mask=True, return_token_type_ids=True)\n",
    "\n",
    "        input_ids.extend(inputs['input_ids'])\n",
    "        attention_mask.extend(inputs['attention_mask'])\n",
    "        token_type_ids.extend(inputs['token_type_ids'])\n",
    "\n",
    "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask), tf.convert_to_tensor(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927439505
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def format_inputs(train_text, train_demographic_vectors, val_text, val_demographic_vectors, test_text, test_demographic_vectors):\n",
    "        \"\"\"\"\"\"\"\"\"\n",
    "    A function that formats train/val/test data by creating batches for all text data\n",
    "    converting demographic vectors to tensors,\n",
    "    and creating encoded demographic vectors by retrieving the endoded version of the vector and converting it to a tensor. \n",
    "    \n",
    "    Input:\n",
    "        - train/val/test_text :                 List of tuples where each tuple represents a label and an interview extract\n",
    "        - train/val/test_demographic_vectors :  List of lists where each list represents a vector of demographic information\n",
    "    Output:\n",
    "        - X_train/val/test_ids:                 sequence of texts encoded as a tf.Tensor object\n",
    "        - X_train/val/test_attention:           the texts' attention mask encoded as a tf.Tensor object\n",
    "        - X_train/val/test_demographic_vectors: demographic vectors as a tf.Tensor object\n",
    "        - X_train/val/test_demographic_vectors_encoded : demographic vectors encoded by autoencoder as a tf.Tensor object\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "        X_train_ids, X_train_attention, X_train_token_type_ids      = batch_encode(tokenizer, train_text)\n",
    "        X_val_ids, X_val_attention, X_val_token_type_ids            = batch_encode(tokenizer, val_text)\n",
    "        X_test_ids, X_test_attention, X_test_token_type_ids         = batch_encode(tokenizer, test_text)\n",
    "\n",
    "        X_train_demographic_vectors_encoded = tf.convert_to_tensor([vectors_to_encoding[tuple(vector)] for vector in train_demographic_vectors])\n",
    "        X_val_demographic_vectors_encoded = tf.convert_to_tensor([vectors_to_encoding[tuple(vector)] for vector in val_demographic_vectors])\n",
    "        X_test_demographic_vectors_encoded = tf.convert_to_tensor([vectors_to_encoding[tuple(vector)] for vector in test_demographic_vectors])\n",
    "\n",
    "        X_train_demographic_vectors       = tf.convert_to_tensor(train_demographic_vectors)\n",
    "        X_val_demographic_vectors         = tf.convert_to_tensor(val_demographic_vectors)\n",
    "        X_test_demographic_vectors        = tf.convert_to_tensor(test_demographic_vectors)\n",
    "\n",
    "        return(X_train_ids, X_train_attention, X_train_token_type_ids, X_train_demographic_vectors, X_train_demographic_vectors_encoded, \\\n",
    "            X_val_ids, X_val_attention, X_val_token_type_ids, X_val_demographic_vectors, X_val_demographic_vectors_encoded, \\\n",
    "            X_test_ids, X_test_attention, X_test_token_type_ids, X_test_demographic_vectors, X_test_demographic_vectors_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927440454
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 49)]              0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 44)                2200      \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 40)                1800      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense4 (Dense)               (None, 36)                1476      \n",
      "_________________________________________________________________\n",
      "encoder_layer (Dense)        (None, 32)                1184      \n",
      "_________________________________________________________________\n",
      "dense5 (Dense)               (None, 36)                1188      \n",
      "_________________________________________________________________\n",
      "dense6 (Dense)               (None, 40)                1480      \n",
      "_________________________________________________________________\n",
      "dense7 (Dense)               (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense8 (Dense)               (None, 44)                1804      \n",
      "_________________________________________________________________\n",
      "decoder_layer (Dense)        (None, 49)                2205      \n",
      "=================================================================\n",
      "Total params: 16,617\n",
      "Trainable params: 16,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1bfaf3f3a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOENCODER_PATH = ''\n",
    "## build autoencoder\n",
    "\n",
    "# Define weight initializer with a random seed\n",
    "weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
    "\n",
    "# This is the input vector\n",
    "input_vector = tf.keras.Input(shape=(VECTOR_LENGTH,))\n",
    "dense1 = layers.Dense(44, activation='relu', name='dense1')(input_vector)\n",
    "dense2 = layers.Dense(40, activation='relu', name='dense2')(dense1)\n",
    "dense3 = layers.Dense(40, activation='relu', name='dense3')(dense2)\n",
    "dense4 = layers.Dense(36, activation='relu', name='dense4')(dense3)\n",
    "#encoded representation of the input vector\n",
    "encoded = layers.Dense(ENCODING_DIM, activation='relu', name='encoder_layer')(dense4)\n",
    "# the reconstruction of the input\n",
    "dense5 = layers.Dense(36, activation='relu', name='dense5')(encoded)\n",
    "dense6 = layers.Dense(40, activation='relu', name='dense6')(dense5)\n",
    "dense7 = layers.Dense(40, activation='relu', name='dense7')(dense6)\n",
    "dense8 = layers.Dense(44, activation='relu', name='dense8')(dense7)\n",
    "decoded = layers.Dense(VECTOR_LENGTH, activation='sigmoid', name = 'decoder_layer')(dense8)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = tf.keras.Model(input_vector, decoded)\n",
    "\n",
    "# This model maps an input to its encoded representation\n",
    "encoder = tf.keras.Model(input_vector, encoded)\n",
    "\n",
    "autoencoder.compile(tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_ENCODER), loss='binary_crossentropy')\n",
    "autoencoder.summary()\n",
    "#load autoencoder weights\n",
    "autoencoder.load_weights(os.path.join(AUTOENCODER_PATH, '/weights'))\n",
    "encoder.load_weights(os.path.join(AUTOENCODER_PATH, '/weights'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "gather": {
     "logged": 1636927440992
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create a list of all unique demographic vectors, so we can encode them all at once\n",
    "demographic_vectors = [list(x) for x in set(tuple(x) for x in (train_demographic_vectors + val_demographic_vectors + test_demographic_vectors))]\n",
    "#encode the vectors\n",
    "encoded_vectors = encoder.predict(demographic_vectors)\n",
    "#save encoded vectors in a dict\n",
    "vectors_to_encoding = {tuple(demvec) : encoded_vectors[n] for n, demvec in enumerate(demographic_vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927557461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#format input data\n",
    "X_train_ids, X_train_attention, X_train_token_type_ids, X_train_demographic_vectors, X_train_demographic_vectors_encoded, \\\n",
    "    X_val_ids, X_val_attention, X_val_token_type_ids, X_val_demographic_vectors, X_val_demographic_vectors_encoded, \\\n",
    "        X_test_ids, X_test_attention, X_test_token_type_ids, X_test_demographic_vectors, X_test_demographic_vectors_encoded = \\\n",
    "            format_inputs(train_text, train_demographic_vectors, val_text, val_demographic_vectors, test_text, test_demographic_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927558455
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "config = DistilBertConfig(output_hidden_states=True)\n",
    "                          \n",
    "# Load the pre-trained DistilBERT transformer model outputting raw hidden-states, without any specific head on top.\n",
    "distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config, name='ourDistilBert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Load evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927558856
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred): # recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # TP\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1))) # P\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred): #precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # TP\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) # TP + FP\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred): #F1-score\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def TP(y_true, y_pred): #true positives\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # TP\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    n_pos = K.sum(y_pos)\n",
    "    y_neg = 1 - y_pos\n",
    "    n_neg = K.sum(y_neg)\n",
    "    n = n_pos + n_neg\n",
    "    return tp/n\n",
    "\n",
    "def TN(y_true, y_pred): #true negatives\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    n_pos = K.sum(y_pos)\n",
    "    y_neg = 1 - y_pos\n",
    "    n_neg = K.sum(y_neg)\n",
    "    n = n_pos + n_neg\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    tn = K.sum(K.round(K.clip(y_neg * y_pred_neg, 0, 1))) # TN\n",
    "    return tn/n\n",
    "\n",
    "def FP(y_true, y_pred): #false positives\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    n_pos = K.sum(y_pos)\n",
    "    y_neg = 1 - y_pos\n",
    "    n_neg = K.sum(y_neg)\n",
    "    n = n_pos + n_neg\n",
    "    tn = K.sum(K.round(K.clip(y_neg * y_pred, 0, 1))) # FP\n",
    "    return tn/n\n",
    "\n",
    "def FN(y_true, y_pred): #false negatives\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    n_pos = K.sum(y_pos)\n",
    "    y_neg = 1 - y_pos\n",
    "    n_neg = K.sum(y_neg)\n",
    "    n = n_pos + n_neg\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    tn = K.sum(K.round(K.clip(y_true * y_pred_neg, 0, 1))) # FN\n",
    "    return tn/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1631087926123
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def build_model(transformer, learning_rate, max_length=MAX_LENGTH):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    This function builds our baseline model that performs the UPV classification task without demographic information\n",
    "    \n",
    "    Input:\n",
    "      - transformer:    DistilBERT transformer model object\n",
    "                        with no added classification head attached.\n",
    "      - learning_rate:  integer controlling the learning rate of the model\n",
    "      - max_length:     integer controlling the maximum number of encoded tokens \n",
    "                        in a given sequence.\n",
    "    \n",
    "    Output:\n",
    "      - model:        a compiled tf.keras.Model with added classification layers \n",
    "                      on top of the base pre-trained model architecture.\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    # Define weight initializer with a random seed to ensure reproducibility\n",
    "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
    "    \n",
    "    # Define input layers\n",
    "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                            name='input_ids', \n",
    "                                            dtype='int32')\n",
    "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                                  name='input_attention', \n",
    "                                                  dtype='int32')\n",
    "    input_token_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                                  name='token_type_ids', \n",
    "                                                  dtype='int32')\n",
    "  \n",
    "    # get the hidden-state at the output of the model's last layer.\n",
    "    last_hidden_state = transformer({'input_ids': input_ids_layer, 'attention_mask': input_attention_layer, 'token_type_ids': input_token_ids_layer})[0]\n",
    "    \n",
    "    # get the [CLS] token\n",
    "    cls_token = last_hidden_state[:, 0, :]\n",
    "    \n",
    "    # Define a single node that makes up the output layer (for binary classification)\n",
    "    output = tf.keras.layers.Dense(1, \n",
    "                                   activation='sigmoid',\n",
    "                                   kernel_initializer=weight_initializer,  \n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros',\n",
    "                                   name='t3_classifier'\n",
    "                                   )(cls_token)\n",
    "    # Define the model\n",
    "    model = tf.keras.Model([input_ids_layer, input_attention_layer, input_token_ids_layer], output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(), f1_m, precision_m, recall_m, TP, TN, FP, FN])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1631087928151
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model_folder, learning_rate) :\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    This function builds the model and trains it \n",
    "    \n",
    "    Input:\n",
    "      - model_folter :      folder in which to save the model\n",
    "      - learning_rate:      integer controlling the learning rate of the model\n",
    "    Output:\n",
    "      - model:        a trained tf.keras.Model \n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    NUM_STEPS = len(X_train_ids) // BATCH_SIZE\n",
    "    #build the model\n",
    "    model=build_model(distilBERT, learning_rate, MAX_LENGTH)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    #add early stopping with patience of 7\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_f1_m\", \n",
    "        min_delta=0,\n",
    "        patience=7,\n",
    "        verbose=1,\n",
    "        mode=\"max\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(MODELS_PATH, model_folder, 'training.log'))\n",
    "\n",
    "    #Train the model\n",
    "    train_history1 = model.fit(\n",
    "        x = [X_train_ids, X_train_attention, X_train_token_type_ids],\n",
    "        y = np.array(train_gold_labels),\n",
    "        epochs = EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        steps_per_epoch = NUM_STEPS,\n",
    "        callbacks= [stop_early, csv_logger],\n",
    "        validation_data=([X_val_ids, X_val_attention, X_val_token_type_ids],  np.array(val_gold_labels)),\n",
    "        verbose=1\n",
    "    )\n",
    "    #save model weights\n",
    "    model.save_weights(os.path.join(MODELS_PATH, model_folder, 'weights'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Regular baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1631087956012
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "BASELINE_PATH = ''\n",
    "baseline_model = train_model(BASELINE_PATH, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1630419794185
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627/627 [==============================] - 129s 206ms/step - loss: 0.0745 - binary_accuracy: 0.9772 - f1_m: 0.6912 - precision_m: 0.6543 - recall_m: 0.8184 - TP: 0.0258 - TN: 0.9513 - FP: 0.0166 - FN: 0.0062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "{'loss': 0.07448320835828781, 'binary_accuracy': 0.9771679043769836, 'f1_m': 0.6911736130714417, 'precision_m': 0.654262125492096, 'recall_m': 0.8183943033218384, 'TP': 0.025825539603829384, 'TN': 0.9513434171676636, 'FP': 0.016625888645648956, 'FN': 0.00620514340698719}\n"
     ]
    }
   ],
   "source": [
    "results_baseline_model = baseline_model.evaluate(\n",
    "                    x = [X_test_ids, X_test_attention, X_test_token_type_ids],\n",
    "                    y = np.array(test_gold_labels), \n",
    "                    return_dict=True, \n",
    "                    batch_size=BATCH_SIZE)\n",
    "print(results_baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Demographic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927587371
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def build_model(transformer, learning_rate, max_length=MAX_LENGTH, vector_length=VECTOR_LENGTH):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    This function builds our demographic model that performs the UPV classification task with included demographic information\n",
    "    \n",
    "    Input:\n",
    "      - transformer:    DistilBERT transformer model object\n",
    "                        with no added classification head attached.\n",
    "      - learning_rate:  integer controlling the learning rate of the model\n",
    "      - max_length:     integer controlling the maximum number of encoded tokens \n",
    "                        in a given sequence.\n",
    "      - vector_length : integer controlling the length of the demographic vector\n",
    "    \n",
    "    Output:\n",
    "      - model:        a compiled tf.keras.Model with added classification layers \n",
    "                      on top of the base pre-trained model architecture.\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    # Define weight initializer with a random seed to ensure reproducibility\n",
    "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
    "    \n",
    "    # Define input layers\n",
    "    demographic_input_layer = tf.keras.layers.Input(shape= (vector_length,),\n",
    "                                            name='demographic_features',\n",
    "                                            dtype='float') \n",
    "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                            name='input_ids', \n",
    "                                            dtype='int32')\n",
    "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                                  name='input_attention', \n",
    "                                                  dtype='int32')\n",
    "    input_token_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                                  name='token_type_ids', \n",
    "                                                  dtype='int32')\n",
    "  \n",
    "    # get the hidden state at the output of the model's last layer: a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
    "    last_hidden_state = transformer({'input_ids': input_ids_layer, 'attention_mask': input_attention_layer, 'token_type_ids': input_token_ids_layer})[0]\n",
    "    \n",
    "    # get DistilBERT's output for the [CLS] token, \n",
    "    cls_token = last_hidden_state[:, 0, :] #dimensionality = 768\n",
    "    \n",
    "    demographic_layer_1 = tf.keras.layers.Dense(128, activation='relu', name='demographic_layer_1')(demographic_input_layer) \n",
    "    dropout_dem_layer_1 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_dem_layer_1')(demographic_layer_1)\n",
    "    demographic_layer_2 = tf.keras.layers.Dense(256, activation='relu', name='demographic_layer_2')(dropout_dem_layer_1)\n",
    "    dropout_dem_layer_2 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_dem_layer_2')(demographic_layer_2)\n",
    "    combined_vector_1 = tf.keras.layers.Concatenate()([cls_token, dropout_dem_layer_2]) #dimensionality = 768 + 256 = 1024\n",
    "    combined_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='combined_dense_layer_1')(combined_vector_1)\n",
    "    dropout_combined_1 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_combined_layer_1')(combined_layer_1)\n",
    "    combined_vector_2 = tf.keras.layers.Concatenate()([cls_token, dropout_combined_1]) #dimensionality = 768 + 512 = 1280\n",
    "    combined_layer_2 = tf.keras.layers.Dense(512, activation='relu', name='combined_dense_layer_2')(combined_vector_2)\n",
    "    dropout_combined_2 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_combined_layer_2')(combined_layer_2)\n",
    "    combined_vector_3 = tf.keras.layers.Concatenate()([cls_token, dropout_combined_2]) #dimensionality = 768 + 512 = 1280\n",
    "    \n",
    "    # Define a single node that makes up the output layer (for binary classification)\n",
    "    output = tf.keras.layers.Dense(1, \n",
    "                                   activation='sigmoid',\n",
    "                                   kernel_initializer=weight_initializer,  \n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros',\n",
    "                                   name='t3_classifier'\n",
    "                                   )(combined_vector_3)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model([demographic_input_layer, input_ids_layer, input_attention_layer, input_token_ids_layer], output)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(), f1_m, precision_m, recall_m, TP, TN, FP, FN])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1636927588840
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model_folder, learning_rate=LEARNING_RATE) :\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    This function builds the model and trains it \n",
    "    \n",
    "    Input:\n",
    "      - model_folder :      folder in which to save the model\n",
    "      - learning_rate:      integer controlling the learning rate of the model\n",
    "    Output:\n",
    "      - model:        a trained tf.keras.Model \n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    NUM_STEPS = len(X_train_ids) // BATCH_SIZE\n",
    "    #build the model\n",
    "    model=build_model(distilBERT, learning_rate, MAX_LENGTH)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    #include early stopping with patience of 7\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_f1_m\",\n",
    "        min_delta=0,\n",
    "        patience=7,\n",
    "        verbose=1,\n",
    "        mode=\"max\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(MODELS_PATH, model_folder, 'training.log'))\n",
    "\n",
    "    #Train the model\n",
    "    train_history1 = model.fit(\n",
    "        x = [X_train_demographic_vectors, X_train_ids, X_train_attention, X_train_token_type_ids],\n",
    "        y = np.array(train_gold_labels),\n",
    "        epochs = EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        steps_per_epoch = NUM_STEPS,\n",
    "        callbacks= [stop_early, csv_logger],\n",
    "        validation_data=([X_val_demographic_vectors, X_val_ids, X_val_attention, X_val_token_type_ids],  np.array(val_gold_labels)),\n",
    "        verbose=1\n",
    "    )\n",
    "    #save model weights\n",
    "    model.save_weights(os.path.join(MODELS_PATH, model_folder, 'weights'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1636927609426
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "DEMOGRAPHIC_MODEL_PATH = ''\n",
    "demographic_model = train_model(DEMOGRAPHIC_MODEL_PATH, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1630561962183
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627/627 [==============================] - 130s 203ms/step - loss: 0.0833 - binary_accuracy: 0.9801 - f1_m: 0.7208 - precision_m: 0.6953 - recall_m: 0.8349 - TP: 0.0249 - TN: 0.9552 - FP: 0.0143 - FN: 0.0056\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "{'loss': 0.09242386370897293, 'binary_accuracy': 0.9781400561332703, 'f1_m': 0.7074137926101685, 'precision_m': 0.6753308773040771, 'recall_m': 0.8340508341789246, 'TP': 0.026270028203725815, 'TN': 0.9518707990646362, 'FP': 0.01609848439693451, 'FN': 0.0057606566697359085}\n"
     ]
    }
   ],
   "source": [
    "results_demographic_model = demographic_model.evaluate(\n",
    "                    x = [X_test_demographic_vectors, X_test_ids, X_test_attention, X_test_token_type_ids],\n",
    "                    y = np.array(test_gold_labels), \n",
    "                    return_dict=True, \n",
    "                    batch_size=BATCH_SIZE)\n",
    "print(results_demographic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Encoded demographic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1631081335406
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def build_model(transformer, learning_rate, max_length=MAX_LENGTH, vector_length=VECTOR_LENGTH):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    This function builds our demographic model that performs the UPV classification task with included demographic information,\n",
    "    where this information is added through encoded vectors\n",
    "    \n",
    "    Input:\n",
    "      - transformer:    DistilBERT transformer model object\n",
    "                        with no added classification head attached.\n",
    "      - learning_rate:  integer controlling the learning rate of the model\n",
    "      - max_length:     integer controlling the maximum number of encoded tokens \n",
    "                        in a given sequence.\n",
    "      - vector_length : integer controlling the length of the demographic vector\n",
    "    \n",
    "    Output:\n",
    "      - model:        a compiled tf.keras.Model with added classification layers \n",
    "                      on top of the base pre-trained model architecture.\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    # Define weight initializer with a random seed to ensure reproducibility\n",
    "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
    "    \n",
    "    # Define input layers\n",
    "    demographic_input_layer = tf.keras.layers.Input(shape= (ENCODING_DIM,),\n",
    "                                            name='demographic_features',\n",
    "                                            dtype='float') \n",
    "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                            name='input_ids', \n",
    "                                            dtype='int32')\n",
    "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                                  name='input_attention', \n",
    "                                                  dtype='int32')\n",
    "    input_token_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                                  name='token_type_ids', \n",
    "                                                  dtype='int32')\n",
    "  \n",
    "     # get the hidden state at the output of the model's last layer: a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
    "    last_hidden_state = transformer({'input_ids': input_ids_layer, 'attention_mask': input_attention_layer, 'token_type_ids': input_token_ids_layer})[0]\n",
    "    \n",
    "    # get DistilBERT's output for the [CLS] token\n",
    "    cls_token = last_hidden_state[:, 0, :] #dimensionality = 768\n",
    "    demographic_layer_1 = tf.keras.layers.Dense(128, activation='relu', name='demographic_layer_1')(demographic_input_layer)\n",
    "    dropout_dem_layer_1 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_dem_layer_1')(demographic_layer_1)\n",
    "    demographic_layer_2 = tf.keras.layers.Dense(256, activation='relu', name='demographic_layer_2')(dropout_dem_layer_1)\n",
    "    dropout_dem_layer_2 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_dem_layer_2')(demographic_layer_2)\n",
    "    demographic_layer_3 = tf.keras.layers.Dense(256, activation='relu', name='demographic_layer_3')(dropout_dem_layer_2)\n",
    "    dropout_dem_layer_3 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_dem_layer_3')(demographic_layer_3)\n",
    "    demographic_layer_4 = tf.keras.layers.Dense(256, activation='relu', name='demographic_layer_4')(dropout_dem_layer_3)\n",
    "    dropout_dem_layer_4 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_dem_layer_4')(demographic_layer_4)\n",
    "    combined_vector_1 = tf.keras.layers.Concatenate()([cls_token, dropout_dem_layer_4]) #dimensionality = 768 + 256 = 1024\n",
    "    combined_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='combined_dense_layer_1')(combined_vector_1)\n",
    "    dropout_combined_1 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_combined_layer_1')(combined_layer_1)\n",
    "    combined_vector_2 = tf.keras.layers.Concatenate()([cls_token, dropout_combined_1]) #dimensionality = 768 + 512 = 1280\n",
    "    combined_layer_2 = tf.keras.layers.Dense(512, activation='relu', name='combined_dense_layer_2')(combined_vector_2)\n",
    "    dropout_combined_2 = tf.keras.layers.Dropout(LAYER_DROPOUT, seed=RANDOM_STATE, name='dropout_combined_layer_2')(combined_layer_2)\n",
    "    combined_vector_3 = tf.keras.layers.Concatenate()([cls_token, dropout_combined_2]) #dimensionality = 768 + 512 = 1280\n",
    "    \n",
    "    # Define a single node that makes up the output layer (for binary classification)\n",
    "    output = tf.keras.layers.Dense(1, \n",
    "                                   activation='sigmoid',\n",
    "                                   kernel_initializer=weight_initializer,  \n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros',\n",
    "                                   name='t3_classifier'\n",
    "                                   )(combined_vector_3)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model([demographic_input_layer, input_ids_layer, input_attention_layer, input_token_ids_layer], output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(), f1_m, precision_m, recall_m, TP, TN, FP, FN])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1631081336362
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "def train_model(model_folder, learning_rate=LEARNING_RATE) :\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    This function builds the model and trains it \n",
    "    \n",
    "    Input:\n",
    "      - model_folder :      folder in which to save the model\n",
    "      - learning_rate:      integer controlling the learning rate of the model\n",
    "    Output:\n",
    "      - model:        a trained tf.keras.Model \n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    NUM_STEPS = len(X_train_ids) // BATCH_SIZE\n",
    "    model=build_model(distilBERT, learning_rate, MAX_LENGTH)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    #add early stopping\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_f1_m\",\n",
    "        min_delta=0,\n",
    "        patience=7,\n",
    "        verbose=1,\n",
    "        mode=\"max\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(MODELS_PATH, model_folder, 'training.log'))\n",
    "\n",
    "    #Train the model\n",
    "    train_history1 = model.fit(\n",
    "        x = [X_train_demographic_vectors_encoded, X_train_ids, X_train_attention, X_train_token_type_ids],\n",
    "        y = np.array(train_gold_labels),\n",
    "        epochs = EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        steps_per_epoch = NUM_STEPS,\n",
    "        callbacks= [stop_early, csv_logger],\n",
    "        validation_data=([X_val_demographic_vectors_encoded, X_val_ids, X_val_attention, X_val_token_type_ids],  np.array(val_gold_labels)),\n",
    "        verbose=1\n",
    "    )\n",
    "    #save model weights\n",
    "    model.save_weights(os.path.join(MODELS_PATH, model_folder, 'weights'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Train regular encoded demographic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1631053584991
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "demographic_features (InputLaye [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demographic_layer_1 (Dense)     (None, 128)          4224        demographic_features[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_dem_layer_1 (Dropout)   (None, 128)          0           demographic_layer_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "demographic_layer_2 (Dense)     (None, 256)          33024       dropout_dem_layer_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_dem_layer_2 (Dropout)   (None, 256)          0           demographic_layer_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "demographic_layer_3 (Dense)     (None, 256)          65792       dropout_dem_layer_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_attention (InputLayer)    [(None, 54)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids (InputLayer)          [(None, 54)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 54)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_dem_layer_3 (Dropout)   (None, 256)          0           demographic_layer_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ourDistilBert (TFDistilBertMode TFBaseModelOutput(la 66362880    input_attention[0][0]            \n",
      "                                                                 input_ids[0][0]                  \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "demographic_layer_4 (Dense)     (None, 256)          65792       dropout_dem_layer_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 768)          0           ourDistilBert[0][7]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_dem_layer_4 (Dropout)   (None, 256)          0           demographic_layer_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 dropout_dem_layer_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "combined_dense_layer_1 (Dense)  (None, 512)          524800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_combined_layer_1 (Dropo (None, 512)          0           combined_dense_layer_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1280)         0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 dropout_combined_layer_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "combined_dense_layer_2 (Dense)  (None, 512)          655872      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_combined_layer_2 (Dropo (None, 512)          0           combined_dense_layer_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1280)         0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 dropout_combined_layer_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "t3_classifier (Dense)           (None, 1)            1281        concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 67,713,665\n",
      "Trainable params: 67,713,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Directory  Models/model_DV_20_dropout_concat_encoded_demographics_new_encodings  Created \n",
      "Epoch 1/25\n",
      "1045/1045 [==============================] - 681s 643ms/step - loss: 0.1163 - binary_accuracy: 0.9656 - f1_m: 0.3145 - precision_m: 0.3843 - recall_m: 0.2969 - TP: 0.0143 - TN: 0.9513 - FP: 0.0043 - FN: 0.0301 - val_loss: 0.0855 - val_binary_accuracy: 0.9731 - val_f1_m: 0.7638 - val_precision_m: 0.8359 - val_recall_m: 0.7365 - val_TP: 0.0460 - val_TN: 0.9272 - val_FP: 0.0103 - val_FN: 0.0165\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/25\n",
      "1045/1045 [==============================] - 671s 642ms/step - loss: 0.0433 - binary_accuracy: 0.9860 - f1_m: 0.7372 - precision_m: 0.7923 - recall_m: 0.7247 - TP: 0.0348 - TN: 0.9512 - FP: 0.0049 - FN: 0.0091 - val_loss: 0.0819 - val_binary_accuracy: 0.9744 - val_f1_m: 0.7917 - val_precision_m: 0.8309 - val_recall_m: 0.7899 - val_TP: 0.0492 - val_TN: 0.9253 - val_FP: 0.0122 - val_FN: 0.0133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/25\n",
      "1045/1045 [==============================] - 673s 644ms/step - loss: 0.0297 - binary_accuracy: 0.9897 - f1_m: 0.8351 - precision_m: 0.8591 - recall_m: 0.8429 - TP: 0.0405 - TN: 0.9492 - FP: 0.0045 - FN: 0.0058 - val_loss: 0.0888 - val_binary_accuracy: 0.9748 - val_f1_m: 0.7928 - val_precision_m: 0.8365 - val_recall_m: 0.7907 - val_TP: 0.0493 - val_TN: 0.9255 - val_FP: 0.0120 - val_FN: 0.0132\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/25\n",
      "1045/1045 [==============================] - 672s 643ms/step - loss: 0.0216 - binary_accuracy: 0.9924 - f1_m: 0.8565 - precision_m: 0.8632 - recall_m: 0.8752 - TP: 0.0409 - TN: 0.9515 - FP: 0.0041 - FN: 0.0036 - val_loss: 0.0967 - val_binary_accuracy: 0.9675 - val_f1_m: 0.7496 - val_precision_m: 0.7550 - val_recall_m: 0.7920 - val_TP: 0.0496 - val_TN: 0.9179 - val_FP: 0.0196 - val_FN: 0.0129\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/25\n",
      "1045/1045 [==============================] - 673s 644ms/step - loss: 0.0181 - binary_accuracy: 0.9935 - f1_m: 0.8564 - precision_m: 0.8646 - recall_m: 0.8701 - TP: 0.0407 - TN: 0.9528 - FP: 0.0035 - FN: 0.0030 - val_loss: 0.1373 - val_binary_accuracy: 0.9692 - val_f1_m: 0.7286 - val_precision_m: 0.8067 - val_recall_m: 0.7111 - val_TP: 0.0444 - val_TN: 0.9248 - val_FP: 0.0127 - val_FN: 0.0181\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/25\n",
      "1045/1045 [==============================] - 674s 645ms/step - loss: 0.0079 - binary_accuracy: 0.9976 - f1_m: 0.9145 - precision_m: 0.9190 - recall_m: 0.9181 - TP: 0.0432 - TN: 0.9544 - FP: 0.0012 - FN: 0.0012 - val_loss: 0.1141 - val_binary_accuracy: 0.9697 - val_f1_m: 0.7435 - val_precision_m: 0.7908 - val_recall_m: 0.7360 - val_TP: 0.0459 - val_TN: 0.9238 - val_FP: 0.0137 - val_FN: 0.0166\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/25\n",
      "1045/1045 [==============================] - 674s 645ms/step - loss: 0.0099 - binary_accuracy: 0.9965 - f1_m: 0.8936 - precision_m: 0.8998 - recall_m: 0.8991 - TP: 0.0433 - TN: 0.9532 - FP: 0.0018 - FN: 0.0017 - val_loss: 0.1211 - val_binary_accuracy: 0.9709 - val_f1_m: 0.7635 - val_precision_m: 0.8009 - val_recall_m: 0.7678 - val_TP: 0.0480 - val_TN: 0.9228 - val_FP: 0.0147 - val_FN: 0.0145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/25\n",
      "1045/1045 [==============================] - 673s 644ms/step - loss: 0.0079 - binary_accuracy: 0.9974 - f1_m: 0.9112 - precision_m: 0.9144 - recall_m: 0.9188 - TP: 0.0428 - TN: 0.9546 - FP: 0.0014 - FN: 0.0012 - val_loss: 0.1822 - val_binary_accuracy: 0.9667 - val_f1_m: 0.7553 - val_precision_m: 0.7601 - val_recall_m: 0.7965 - val_TP: 0.0497 - val_TN: 0.9170 - val_FP: 0.0205 - val_FN: 0.0128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/25\n",
      "1045/1045 [==============================] - 673s 644ms/step - loss: 0.0082 - binary_accuracy: 0.9976 - f1_m: 0.9125 - precision_m: 0.9180 - recall_m: 0.9151 - TP: 0.0440 - TN: 0.9536 - FP: 0.0012 - FN: 0.0011 - val_loss: 0.1370 - val_binary_accuracy: 0.9659 - val_f1_m: 0.7406 - val_precision_m: 0.7490 - val_recall_m: 0.7837 - val_TP: 0.0492 - val_TN: 0.9167 - val_FP: 0.0208 - val_FN: 0.0133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "ENC_DEM_PATH = ''\n",
    "encoded_demographic_model = train_model(ENC_DEM_PATH, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1631053714252
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627/627 [==============================] - 129s 206ms/step - loss: 0.0585 - binary_accuracy: 0.9805 - f1_m: 0.7201 - precision_m: 0.6933 - recall_m: 0.8252 - TP: 0.0259 - TN: 0.9546 - FP: 0.0134 - FN: 0.0061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "{'loss': 0.05849744752049446, 'binary_accuracy': 0.9805329442024231, 'f1_m': 0.7200949192047119, 'precision_m': 0.6933411955833435, 'recall_m': 0.8251991271972656, 'TP': 0.02592114545404911, 'TN': 0.9546120166778564, 'FP': 0.013357256539165974, 'FN': 0.0061095403507351875}\n"
     ]
    }
   ],
   "source": [
    "results_encoded_demographic_model = encoded_demographic_model.evaluate(\n",
    "                    x = [X_test_demographic_vectors_encoded, X_test_ids, X_test_attention, X_test_token_type_ids],\n",
    "                    y = np.array(test_gold_labels), \n",
    "                    return_dict=True, \n",
    "                    batch_size=BATCH_SIZE)\n",
    "print(results_encoded_demographic_model)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
